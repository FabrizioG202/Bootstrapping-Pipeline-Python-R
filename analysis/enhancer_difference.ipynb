{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.clus_files_io import parse_clus_file\n",
    "from src.cluster_description import ClustersDescription\n",
    "import pyranges as pr \n",
    "import pandas as pd\n",
    "import os,json\n",
    "import numpy as np\n",
    "from src.utils import RESOLUTIONS, extract_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cluster which are Enriched in Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the information about the cluster enriched in both CAGE and CTCF\n",
    "CLUSTERS_IN_ANALYSIS_PATH = \"../analysis_results/ctcf.CLUS\"\n",
    "\n",
    "# Reading the clusters:\n",
    "clusters_in_analysis = parse_clus_file(CLUSTERS_IN_ANALYSIS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Feature Types File\n",
    "Read the File mapping each Feature to its Type, convert them to ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_TYPE_PATH = \"./data/FANTOM5_cage_peak_type_tbl.tsv\"\n",
    "\n",
    "### Parses the file containing the peak types for each feature and returns a dictionary with the feature id as key and the feature type as value\n",
    "### Like this:\n",
    "### {\n",
    "###    \"peak_1\": \"tss\",\n",
    "###    \"peak_2\": \"tss\",\n",
    "###    \"peak_3\": \"enhancer\",\n",
    "###    \"peak_4\": \"tss\",\n",
    "###     ...\n",
    "### }\n",
    "def parse_peak_type(path : str) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the peak type table.\n",
    "    \"\"\"\n",
    "    peak_type = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        # consume the first line of the line iterators to skip the header\n",
    "        header = f.readline().strip()\n",
    "\n",
    "        # loop over the header\n",
    "        for line in f.readlines():\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            peak_type[line[0]] = line[1]\n",
    "    return peak_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_types = parse_peak_type(FEATURE_TYPE_PATH)\n",
    "# make them into a dataframe \n",
    "peak_types_df = pd.DataFrame.from_dict(peak_types, orient=\"index\")\n",
    "peak_types_df.columns = [\"peak_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chr_start_end(x : str) -> tuple[str, int, int]:\n",
    "    \"\"\"\n",
    "    Extract the chromosome and the start and end of the feature.\n",
    "    \"\"\"\n",
    "    chromo = x.split(\":\")[0]\n",
    "    remaindeer =  x.split(\":\")[1].split(\",\")[0]\n",
    "    split_ = \"..\" if \"..\" in remaindeer else \"-\"\n",
    "    start, end = remaindeer.split(split_)\n",
    "    return chromo, int(start), int(end), x\n",
    "\n",
    "# Transform them to a dataframe and then to a pyranges object\n",
    "peak_types_df[\"chromo\"], peak_types_df[\"start\"], peak_types_df[\"end\"], names = zip(*peak_types_df.index.map(extract_chr_start_end))\n",
    "peak_types_df[\"name\"] = names\n",
    "peak_types_df.rename({\"chromo\" : \"Chromosome\", \"start\" : \"Start\", \"end\" : \"End\", \"name\" : \"Name\"}, axis=1, inplace=True)\n",
    "\n",
    "# Drop the index\n",
    "peak_types_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter the enhancers\n",
    "ENHANCERS = peak_types_df[peak_types_df[\"peak_type\"] == \"enhancer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Enhancer Length: 281.14\n"
     ]
    }
   ],
   "source": [
    "ENHANCERS_RANGES = pr.PyRanges(ENHANCERS)\n",
    "print(\"Average Enhancer Length: {:.2f}\".format(ENHANCERS_RANGES.lengths().mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the clusters which we are interested in\n",
    "Cluster out are the clusters which represent the non-enriched clusters, which are not enriched in CTCF and CAGE.\n",
    "\n",
    "Cluster in are the clusters which represent the enriched clusters, which are enriched in CTCF and CAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_clusters(cluster_folder : str, include_dict : dict[str, str] = None, exclude_dict : dict[str, str] = None) -> pr.PyRanges:\n",
    "    for file in os.listdir(cluster_folder):\n",
    "        chromo = file.split(\"_\")[0]\n",
    "        \n",
    "        if include_dict and chromo not in include_dict:\n",
    "            continue\n",
    "        \n",
    "        clusters = json.load(open(os.path.join(cluster_folder, file), \"r\"))\n",
    "        clusters = clusters[\"cl_member\"]\n",
    "        for cluster_id, bins in clusters.items():\n",
    "            resolution = int(RESOLUTIONS[cluster_id.split(\"_\")[0]])\n",
    "\n",
    "            # Deal with inclusion\n",
    "            if include_dict:\n",
    "                if cluster_id not in include_dict.get(chromo, []):\n",
    "                    continue\n",
    "\n",
    "            # Deal with exclusion\n",
    "            if exclude_dict:\n",
    "                if cluster_id in exclude_dict.get(chromo, []):\n",
    "                    continue\n",
    "                \n",
    "            for bin in extract_bins(bins):            \n",
    "                yield chromo, bin, bin + resolution, cluster_id\n",
    "\n",
    "\n",
    "# Loading the clusters\n",
    "CLUSTER_FOLDER = \"../data/clusters/HMEC/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chromosomes, Starts and ends.\n",
    "chromosomes = []\n",
    "starts = []\n",
    "ends = []\n",
    "names = []\n",
    "chromosomes, starts, ends, names = zip(*import_clusters(CLUSTER_FOLDER, include_dict=clusters_in_analysis))\n",
    "\n",
    "FOREGROUND = pr.PyRanges(pd.DataFrame({\"Chromosome\":chromosomes, \"Start\":starts, \"End\":ends, \"name\":names}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = \"../results/\"\n",
    "\n",
    "# Saving all the enhancer as a background\n",
    "ENHANCERS[[\"Chromosome\", \"Start\", \"End\"]].to_csv(os.path.join(RESULTS_PATH, \"background.bed\"), sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "# Overlapping the enhancers with the clusters and saving the results as a bed.\n",
    "enhancer_in_clusters = ENHANCERS_RANGES.overlap(FOREGROUND)\n",
    "enhancer_in_clusters.df[[\"Chromosome\", \"Start\", \"End\"]].to_csv(os.path.join(RESULTS_PATH, \"foreground.bed\"), sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for checking enrichment with unibind.\n",
    "To prepare the data for use with unibind, we need to convert the coordinates to hg38 for both the foreground and the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Converting the foreground\n",
      "Size Before Conversion:  2281\n",
      "Coordinates translated, new size:  2290\n",
      "-- Converting the background\n",
      "Size Before Conversion:  65423\n",
      "Coordinates translated, new size:  65585\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def convert_to_hg38(path : str):\n",
    "    # Call the script ./analysis/convert_to_hg38.R \n",
    "    # make the path absolute\n",
    "    path = os.path.abspath(path)\n",
    "    cmd = 'Rscript ../analysis/scripts/hg19_to_hg38.R \"{}\" \"{}\"'.format(path, path) # The output file will be the same as the input file (inplace)\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        shell=True,\n",
    "        encoding='utf-8',\n",
    "        errors='replace'\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        realtime_output = process.stdout.readline()\n",
    "\n",
    "        if realtime_output == '' and process.poll() is not None:\n",
    "            break\n",
    "\n",
    "        if realtime_output:\n",
    "            print(realtime_output.strip(), flush=True)\n",
    "            pass\n",
    "\n",
    "# Converting the stuff to hg38\n",
    "print(\"-- Converting the foreground\")\n",
    "convert_to_hg38(os.path.join(RESULTS_PATH, \"foreground.bed\"))\n",
    "print(\"-- Converting the background\")\n",
    "convert_to_hg38(os.path.join(RESULTS_PATH, \"background.bed\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6a9dc70ef1d9ac109a6adb4c16e68d584d15afe2d26145e4986863f3d6029bd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cluster_description import Cluster, ClustersDescription\n",
    "import pyranges as pr\n",
    "import numpy as np\n",
    "\n",
    "_GMT_PATH = \"./data/c5.go.bp.v7.5.1.entrez.gmt\"\n",
    "_FANTOM_PATH = \"./data/FANTOM5_CAGE_peak_entrez_gene_tbl.tsv\"\n",
    "\n",
    "# Parses the GMT file returining a dict containing for each \n",
    "# Returns a dictionary mapping each gene entrez id to the ontology group it belongs to.\n",
    "def parse_gmt(path: str) -> dict[str, list[str]]:\n",
    "    # Create a dictionary to store the results\n",
    "    results = {}\n",
    "\n",
    "    # Open the GMT file\n",
    "    with open(path, \"r\") as f:\n",
    "        # For each line in the file\n",
    "        for line in f:\n",
    "            # Split the line into a list of words\n",
    "            words = line.split(\"\\t\")\n",
    "\n",
    "            # The motif name\n",
    "            gene_set = words[0]\n",
    "            link = words[1]\n",
    "            genes = words[2:]\n",
    "            # strip each gene\n",
    "            genes = [gene.strip() for gene in genes]\n",
    "\n",
    "            # Add the gene set to the dictionary\n",
    "            results[gene_set] = [link, genes]\n",
    "\n",
    "    # invert the dictionary\n",
    "    gmt_inverse : dict[str, list[str]] = {}\n",
    "\n",
    "    # For each gene set\n",
    "    for gene_set, (link, genes) in results.items():\n",
    "        for gene in genes:\n",
    "            if gene not in gmt_inverse:\n",
    "                gmt_inverse[gene] = []\n",
    "            gmt_inverse[gene].append(gene_set)\n",
    "        \n",
    "    return gmt_inverse, list(results.keys())\n",
    "\n",
    "# read the table from the path\n",
    "def parse_fantom_data(path: str) -> dict[str, str]:\n",
    "    # Create a dictionary to store the results\n",
    "    results: dict[str, str] = {}\n",
    "\n",
    "    header_found = False\n",
    "\n",
    "    # Open the file\n",
    "    with open(path, \"r\") as f:\n",
    "        # For each line in the file\n",
    "        for line in f:\n",
    "            # Go to the next line if it starts with a #\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # deal with the header\n",
    "            if not header_found:\n",
    "                header_found = True\n",
    "                continue\n",
    "\n",
    "            # Split the line into a list of words\n",
    "            words = line.split(\"\\t\")\n",
    "            cage_id = words[0].strip()\n",
    "            entrez_id = words[1].strip()\n",
    "\n",
    "            # Add the gene to the dictionary\n",
    "            results[cage_id] = entrez_id\n",
    "        \n",
    "    # Return the results\n",
    "    return results\n",
    "\n",
    "# Maps the given entry id to the gene entrez id\n",
    "def overlaps_to_entrez(overlaps: list[str], peak_to_entrez : dict[str, str]) -> list[str]:\n",
    "    entrez_ids = []\n",
    "    for overlap in overlaps:\n",
    "        entrez_ids.append(peak_to_entrez.get(overlap, None))\n",
    "    \n",
    "    # Remove duplicates and NAs\n",
    "    entrez_ids = [entrez_id for entrez_id in entrez_ids if entrez_id not in [\"NA\", None]]\n",
    "    return entrez_ids\n",
    "\n",
    "# Return the ontology group to which each entrez id belongs to.\n",
    "def map_entrez_to_ontology(entrez_ids: list[str], gene_ontology : dict[str, str]) -> list[str]:\n",
    "    ontology_hist = {}\n",
    "    unknown_entrez_ids = []\n",
    "    for entrez_id in entrez_ids:\n",
    "        if entrez_id not in gene_ontology:\n",
    "            unknown_entrez_ids.append(entrez_id)\n",
    "            continue\n",
    "        ontology_groups = gene_ontology[entrez_id]\n",
    "\n",
    "        for ontology_group in ontology_groups:\n",
    "            if ontology_group not in ontology_hist:\n",
    "                ontology_hist[ontology_group] = 0\n",
    "            ontology_hist[ontology_group] += 1\n",
    "    \n",
    "    return ontology_hist, unknown_entrez_ids\n",
    "\n",
    "\n",
    "### Maps the provided features (passed in as the list of names to the the given ontology groups) and returns an histogram of the counts of each group.\n",
    "### Like this:\n",
    "### {   \n",
    "###     \"GO:0005737\": 1,\n",
    "###     \"GO:0005739\": 1,\n",
    "###     \"GO:0005740\": 3,\n",
    "###     ...\n",
    "### }\n",
    "### The features are expected to be already void of duplicates as it would lead to a double counting.\n",
    "def get_groups_for_features(features: list[str], peak_to_entrez: dict[str, str], gene_ontology : dict[str, str]) -> dict[str, int]:\n",
    "    # Get the entrez ids\n",
    "    entrez_ids = overlaps_to_entrez(features, peak_to_entrez)\n",
    "\n",
    "    # Get the ontology\n",
    "    groups, _ = map_entrez_to_ontology(entrez_ids, gene_ontology)\n",
    "    \n",
    "    # Return the groups\n",
    "    return groups, len(entrez_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Imports and Parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data.\n",
    "peak_to_entrez = parse_fantom_data(_FANTOM_PATH)\n",
    "gene_ontology, gene_sets = parse_gmt(_GMT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Background Data.\n",
    "The gene sets counts are computed for all the features present in the data folder to produce a background distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the background\n",
    "background = pr.read_bed(f\"../data/features/HMEC/CAGE/features.bed\")\n",
    "\n",
    "# Getting the representation \n",
    "feature_names = background.Name.values.flatten()\n",
    "expected_groups, population_size = get_groups_for_features(feature_names, peak_to_entrez, gene_ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Observed Data\n",
    "The data about which gene set are enriched in the clusters is generated.\n",
    "To do so, the clusters in analysis are read from the folder. We then find the clusters features in the feature file which overlap each cluster and see which of gene set are present in those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the information about the cluster enriched in both CAGE and CTCF\n",
    "CLUSTERS_IN_ANALYSIS_PATH = \"../results/intersected.tsv\"\n",
    "\n",
    "def parse_enrichment_file(path : str):\n",
    "    results = {}\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            words = line.split(\"\\t\")\n",
    "            chromosome = words[0].strip()\n",
    "            clusters = [w.strip() for w in words[1:]]\n",
    "\n",
    "            results[chromosome] = clusters\n",
    "\n",
    "    return results\n",
    "\n",
    "# Reading the clusters:\n",
    "clusters_in_analysis = parse_enrichment_file(CLUSTERS_IN_ANALYSIS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the features.\n",
    "features = pr.read_bed(\"../data/features/HMEC/CAGE/features.bed\")\n",
    "\n",
    "# the list keeping track of which genes are present in the features in analysis.\n",
    "observed_genes = []\n",
    "\n",
    "# For each cluster\n",
    "for chromo, clusters_names in clusters_in_analysis.items():\n",
    "    # Reading the clusters.\n",
    "    clusters = ClustersDescription(f\"../data/clusters/HMEC/{chromo}_spec_res.json\", chromo)\n",
    "    \n",
    "    # Looping over the cluster (strings)\n",
    "    for cluster in clusters_names:\n",
    "\n",
    "        # Getting the actual cluster object\n",
    "        cluster = clusters[cluster]\n",
    "\n",
    "        # Counting the overlaps\n",
    "        overlaps = cluster.find_overlaps(chromo, features)\n",
    "        overlaps_names = overlaps.Name.values.flatten()\n",
    "        \n",
    "        # Getting the observed genes\n",
    "        observed_genes.extend(overlaps_to_entrez(overlaps_names, peak_to_entrez))\n",
    "\n",
    "        # removing duplicates\n",
    "        observed_genes = list(set(observed_genes))\n",
    "\n",
    "# Getting an histogram of the observed gene groups.\n",
    "observed_groups, _ = map_entrez_to_ontology(observed_genes, gene_ontology)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing the hypergeometric test:\n",
    "The hypergeometric test is performed on the background distribution and the observed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7658/7658 [01:10<00:00, 108.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import hypergeom\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The p_values\n",
    "p_values = {}\n",
    "\n",
    "# Looping over all the gene set in the ontology\n",
    "for _set in tqdm(gene_sets):\n",
    "    \n",
    "    # run the hypergeometric test\n",
    "    M = population_size # population size\n",
    "    n = expected_groups.get(_set, 0) # number of successes in the population\n",
    "    N = len(observed_genes) # size of the sample\n",
    "    x = observed_groups.get(_set, 0) # number of successes in the sample\n",
    "\n",
    "    p_value = hypergeom.sf(x-1, M, n, N)\n",
    "    p_values[_set] = [p_value, x, n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values to a dataframe\n",
    "import pandas as pd\n",
    "p_values_df = pd.DataFrame(p_values).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the column names\n",
    "p_values_df.columns = [\"p_value\", \"observed\", \"background\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply fdr correction\n",
    "import statsmodels.stats.multitest as smm\n",
    "\n",
    "#adding a column called q_value\n",
    "p_values_df[\"q_value\"] = smm.multipletests(p_values_df[\"p_value\"], method=\"fdr_bh\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe on q_value\n",
    "p_values_df.sort_values(\"q_value\", inplace=True)\n",
    "\n",
    "# Add index as a column\n",
    "p_values_df[\"gene_set\"] = p_values_df.index\n",
    "\n",
    "# Make the gene_set the first column\n",
    "p_values_df = p_values_df[[\"gene_set\", \"q_value\", \"p_value\", \"observed\", \"background\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the first 100 results\n",
    "p_values_df[:100].to_csv(\"../results/enrichment_results.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6a9dc70ef1d9ac109a6adb4c16e68d584d15afe2d26145e4986863f3d6029bd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
